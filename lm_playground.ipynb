{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fe523821",
      "metadata": {
        "id": "fe523821"
      },
      "source": [
        "\n",
        "# Project 1: Build an LLM Playground\n",
        "\n",
        "Welcome! In this project, you’ll learn foundations of large language models (LLMs). We’ll keep the code minimal and the explanations high‑level so that anyone who can run a Python cell can follow along.  \n",
        "\n",
        "We'll be using Google Colab for this project. Colab is a free, browser-based platform that lets you run Python code and machine learning models without installing anything on your local computer. Click the button below to open this notebook directly in Google Colab and get started!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdb8584e",
      "metadata": {
        "id": "fdb8584e"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bytebyteai/ai-eng-projects/blob/main/project_1/lm_playground.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08e82492",
      "metadata": {
        "id": "08e82492"
      },
      "source": [
        "---\n",
        "## Learning Objectives  \n",
        "* **Tokenization** and how raw text is tokenized into a sequene of discrete tokens\n",
        "* Inspect **GPT2** and **Transformer architecture**\n",
        "* Loading pre-trained LLMs using **Hugging Face**\n",
        "* **Decoding strategies** to generate text from LLMs\n",
        "* Completion versus **intrusction fine-tuned** LLMs\n",
        "\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1235110e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1235110e",
        "outputId": "0b048d6d-b9ef-409c-e620-d8f38f1c6ae8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\scott\\llm_playground\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch 2.9.0+cpu | transformers 4.57.1\n"
          ]
        }
      ],
      "source": [
        "import torch, transformers, tiktoken\n",
        "print(\"torch\", torch.__version__, \"| transformers\", transformers.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4c1eb0b",
      "metadata": {
        "id": "d4c1eb0b"
      },
      "source": [
        "# 1 - Tokenization\n",
        "\n",
        "A neural network can’t digest raw text. They need **numbers**. Tokenization is the process of converting text into IDs. In this section, you'll learn how tokenization is implemented in practice.\n",
        "\n",
        "Tokenization methods generally fall into three categories:\n",
        "1. Word-level\n",
        "2. Character-level\n",
        "3. Subword-level"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d234dc0",
      "metadata": {
        "id": "1d234dc0"
      },
      "source": [
        "### 1.1 - Word‑level tokenization\n",
        "\n",
        "Split text on whitespace and store each **word** as a token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d784a288",
      "metadata": {
        "id": "d784a288"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 21 words\n",
            "First 15 vocab entries: ['[PAD]', '[UNK]', 'brown', 'converts', 'dog', 'fox', 'jumps', 'language', 'large', 'lazy', 'models', 'next', 'numbers', 'over', 'predict']\n",
            "\n",
            "Input text : The brown unicorn jumps\n",
            "Token IDs  : [17, 2, 1, 6]\n",
            "Decoded    : the brown [UNK] jumps\n"
          ]
        }
      ],
      "source": [
        "# 1. Tiny corpus\n",
        "corpus = [\n",
        "    \"The quick brown fox jumps over the lazy dog\",\n",
        "    \"Tokenization converts text to numbers\",\n",
        "    \"Large language models predict the next token\"\n",
        "]\n",
        "\n",
        "# 2. Build the vocabulary\n",
        "PAD, UNK = \"[PAD]\", \"[UNK]\"\n",
        "words = set()\n",
        "for doc in corpus:\n",
        "    words.update(doc.lower().split())\n",
        "\n",
        "vocab = [PAD, UNK] + sorted(words)\n",
        "word2id = {w: i for i, w in enumerate(vocab)}\n",
        "id2word = {i: w for w, i in word2id.items()}\n",
        "\n",
        "print(f\"Vocabulary size: {len(vocab)} words\")\n",
        "print(\"First 15 vocab entries:\", vocab[:15])\n",
        "\n",
        "# 3. Encode / decode\n",
        "def encode(text):\n",
        "    return [word2id.get(w, word2id[UNK]) for w in text.lower().split()]\n",
        "\n",
        "def decode(ids):\n",
        "    return \" \".join(id2word[i] for i in ids if i != word2id[PAD])\n",
        "\n",
        "# 4. Demo\n",
        "sample = \"The brown unicorn jumps\"\n",
        "ids = encode(sample)\n",
        "recovered = decode(ids)\n",
        "\n",
        "print(\"\\nInput text :\", sample)\n",
        "print(\"Token IDs  :\", ids)\n",
        "print(\"Decoded    :\", recovered)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0edab2c2",
      "metadata": {
        "id": "0edab2c2"
      },
      "source": [
        "Word-level tokenization has two major limitations:\n",
        "1. Large vocabulary size\n",
        "2. Out-of-vocabulary (OOV) issue"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a379bac7",
      "metadata": {
        "id": "a379bac7"
      },
      "source": [
        "### 1.2 - Character‑level tokenization\n",
        "\n",
        "Every single character (including spaces and emojis) gets its own ID. This guarantees zero out‑of‑vocabulary issues but very long sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4ac29144",
      "metadata": {
        "id": "4ac29144"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 54 (52 letters + 2 specials)\n",
            "\n",
            "Input text : Hello\n",
            "Token IDs  : [35, 6, 13, 13, 16]\n",
            "Decoded    : Hello\n"
          ]
        }
      ],
      "source": [
        "# 1. Build a fixed vocabulary # a–z + A–Z + padding + unkwown\n",
        "import string\n",
        "\n",
        "letters = list(string.ascii_lowercase + string.ascii_uppercase)  # a–z + A–Z\n",
        "special = [\"[PAD]\", \"[UNK]\"]  # padding + unknown\n",
        "vocab = special + letters\n",
        "\n",
        "char2id = {ch: idx for idx, ch in enumerate(vocab)}\n",
        "id2char = {idx: ch for ch, idx in char2id.items()}\n",
        "\n",
        "print(f\"Vocabulary size: {len(vocab)} (52 letters + 2 specials)\")\n",
        "\n",
        "# 2. Encode / decode\n",
        "def encode(text):\n",
        "    \"\"\"Convert text → list of IDs (unknown chars → [UNK]).\"\"\"\n",
        "    unk_id = char2id[\"[UNK]\"]\n",
        "    return [char2id.get(ch, unk_id) for ch in text]\n",
        "\n",
        "def decode(ids):\n",
        "    \"\"\"Convert list of IDs.\"\"\"\n",
        "    return \"\".join(id2char[i] for i in ids if i != char2id[\"[PAD]\"])\n",
        "\n",
        "# 3. Demo\n",
        "sample = \"Hello\"\n",
        "ids = encode(sample)\n",
        "recovered = decode(ids)\n",
        "\n",
        "print(\"\\nInput text :\", sample)\n",
        "print(\"Token IDs  :\", ids)\n",
        "print(\"Decoded    :\", recovered)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "391275bd",
      "metadata": {
        "id": "391275bd"
      },
      "source": [
        "### 1.3 - Subword‑level tokenization\n",
        "\n",
        "Sub-word methods such as `Byte-Pair Encoding (BPE)`, `WordPiece`, and `SentencePiece` **learn** the most common character and gorup them into new tokens. For example, the word `unbelievable` might turn into three tokens: `[\"un\", \"believ\", \"able\"]`. This approach strikes a balance between word-level and character-level methods and fix their limitations.\n",
        "\n",
        "For example, `BPE` algorithm forms the vocabulary using the following steps:\n",
        "1. **Start with bytes** → every character is its own token.  \n",
        "2. **Count all adjacent pairs** in a huge corpus.  \n",
        "3. **Merge the most frequent pair** into a new token.  \n",
        "   *Repeat steps 2-3* until you hit the target vocab size (e.g., 50 k).\n",
        "\n",
        "Let's see `BPE` in practice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4675e67a",
      "metadata": {
        "id": "4675e67a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\scott\\llm_playground\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\scott\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size: 50257\n",
            "Special tokens: ['<|endoftext|>']\n",
            "\n",
            "Input text : Unbelievable tokenization powers! 🚀\n",
            "Token IDs  : [3118, 6667, 11203, 540, 11241, 1634, 5635, 0, 12520, 248, 222]\n",
            "Tokens     : ['Un', 'bel', 'iev', 'able', 'Ġtoken', 'ization', 'Ġpowers', '!', 'ĠðŁ', 'ļ', 'Ģ']\n",
            "Decoded    : Unbelievable tokenization powers! 🚀\n"
          ]
        }
      ],
      "source": [
        "# 1. Load a pretrained BPE tokenizer (GPT-2 uses BPE).\n",
        "# Refer to  https://huggingface.co/docs/transformers/en/fast_tokenizers\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "bpe_tok = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "print(\"Vocab size:\", bpe_tok.vocab_size)\n",
        "print(\"Special tokens:\", bpe_tok.all_special_tokens)\n",
        "\n",
        "# 2. Encode / decode\n",
        "def encode(text):\n",
        "    return bpe_tok.encode(text)\n",
        "\n",
        "def decode(ids):\n",
        "    return bpe_tok.decode(ids)\n",
        "\n",
        "# 3. Demo\n",
        "sample = \"Unbelievable tokenization powers! 🚀\"\n",
        "ids = encode(sample)\n",
        "recovered = decode(ids)\n",
        "\n",
        "print(\"\\nInput text :\", sample)\n",
        "print(\"Token IDs  :\", ids)\n",
        "print(\"Tokens     :\", bpe_tok.convert_ids_to_tokens(ids))\n",
        "print(\"Decoded    :\", recovered)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "badaa5a8",
      "metadata": {
        "id": "badaa5a8"
      },
      "source": [
        "### 1.4 - TikToken\n",
        "\n",
        "`tiktoken` is a production-ready library which offers high‑speed tokenization used by OpenAI models.  \n",
        "Let's compare the older **gpt2** encoding with the newer **cl100k_base** used in GPT‑4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "7704c470",
      "metadata": {
        "id": "7704c470"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== gpt2 ===\n",
            "Vocabulary size: 50257\n",
            "Sentence splits into 11 tokens:\n",
            "[('The', 464), (' �', 12520), ('�', 234), ('�', 253), (' player', 2137), (' scored', 7781), (' 400', 7337), (' points', 2173), (' h', 289), ('uzz', 4715), ('ah', 993)]\n",
            "Sample tokens from the vocabulary:\n",
            "[('!', 0), ('\"', 1), ('#', 2), ('\\n', 198), ('<|endoftext|>', 50256)]\n",
            "\n",
            "=== cl100k_base ===\n",
            "Vocabulary size: 100277\n",
            "Sentence splits into 12 tokens:\n",
            "[('The', 791), (' �', 11410), ('�', 234), ('�', 253), (' player', 2851), (' scored', 16957), (' ', 220), ('400', 3443), (' points', 3585), (' h', 305), ('uzz', 9065), ('ah', 1494)]\n",
            "Sample tokens from the vocabulary:\n",
            "[('!', 0), ('\"', 1), ('#', 2), ('\\n', 198), ('parable', 50256)]\n"
          ]
        }
      ],
      "source": [
        "# Use gpt2 and cl100k_base to encode and decode the following text\n",
        "# Refer to https://github.com/openai/tiktoken\n",
        "import tiktoken\n",
        "\n",
        "encodings = [\n",
        "    (\"gpt2\", tiktoken.get_encoding(\"gpt2\")),\n",
        "    (\"cl100k_base\", tiktoken.get_encoding(\"cl100k_base\")),\n",
        "]\n",
        "\n",
        "sentence = \"The 🌟 player scored 400 points huzzah\"\n",
        "\n",
        "for name, enc in encodings:\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    print(\"Vocabulary size:\", enc.n_vocab)\n",
        "\n",
        "    # Encode the sample sentence\n",
        "    ids = enc.encode(sentence)\n",
        "    tokens = [enc.decode([i]) for i in ids]\n",
        "    print(f\"Sentence splits into {len(ids)} tokens:\")\n",
        "    print(list(zip(tokens, ids)))\n",
        "\n",
        "    # Show a few arbitrary token→ID examples from the vocab\n",
        "    some_ids = [0, 1, 2, 198, 50256]\n",
        "    print(\"Sample tokens from the vocabulary:\")\n",
        "    print([(enc.decode([i]), i) for i in some_ids])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e8c1023",
      "metadata": {
        "id": "5e8c1023"
      },
      "source": [
        "Experiment: try new sentences, emojis, code snippets, or other languages. If you are interested, try implementing the BPE algorithm yourself.\n",
        "\n",
        "### 1.5 - Key Takeaways\n",
        "\n",
        "* **Word‑level**: simple but brittle (OOV problems).  \n",
        "* **Character‑level**: robust but produces long sequences.  \n",
        "* **BPE / Byte‑Level BPE**: middle ground used by most LLMs.  \n",
        "* **tiktoken**: shows how production models tokenize with pre‑trained sub‑word vocabularies."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2a758ba",
      "metadata": {
        "id": "c2a758ba"
      },
      "source": [
        "# 2. What is a Language Model?\n",
        "\n",
        "At its core, a **language model (LM)** is just a *very large* mathematical function built from many neural-network layers.  \n",
        "Given a sequence of tokens `[t₁, t₂, …, tₙ]`, it learns to output a probability for the next token `tₙ₊₁`.\n",
        "\n",
        "\n",
        "Each layer applies a simple operation (matrix multiplication, attention, etc.). Stacking hundreds of these layers lets the model capture patterns and statistical relations from text. The final output is a vector of scores that says, “how likely is each possible token to come next?”\n",
        "\n",
        "> Think of the whole network as **one gigantic equation** whose parameters were tuned during training to minimize prediction error.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f0c7399",
      "metadata": {
        "id": "8f0c7399"
      },
      "source": [
        "\n",
        "### 2.1 - A Single `Linear` Layer\n",
        "\n",
        "Before we explore Transformer, let’s start tiny:\n",
        "\n",
        "* A **Linear layer** performs `y = Wx + b`  \n",
        "  * `x` – input vector  \n",
        "  * `W` – weight matrix (learned)  \n",
        "  * `b` – bias vector (learned)\n",
        "\n",
        "Although this looks basic, chaining thousands of such linear transforms (with nonlinearities in between) gives neural nets their expressive power.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "e425948a",
      "metadata": {
        "id": "e425948a"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "class Linear(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(Linear, self).__init__()\n",
        "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
        "        self.bias = nn.Parameter(torch.randn(out_features))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.matmul(x, self.weight.t()) + self.bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "13e5e225",
      "metadata": {
        "id": "13e5e225"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input : tensor([ 1.0000, -1.0000,  0.5000])\n",
            "Weights: Parameter containing:\n",
            "tensor([[-0.5624,  0.0721,  0.2643],\n",
            "        [ 0.2559,  0.5025,  0.1605]], requires_grad=True)\n",
            "Bias   : Parameter containing:\n",
            "tensor([0.4707, 0.2767], requires_grad=True)\n",
            "Output : tensor([-0.0316,  0.1104], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn, torch\n",
        "\n",
        "lin = nn.Linear(3, 2)\n",
        "x = torch.tensor([1.0, -1.0, 0.5])\n",
        "print(\"Input :\", x)\n",
        "print(\"Weights:\", lin.weight)\n",
        "print(\"Bias   :\", lin.bias)\n",
        "print(\"Output :\", lin(x))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a04f56bf",
      "metadata": {
        "id": "a04f56bf"
      },
      "source": [
        "### 2.2 - A `Transformer` Layer\n",
        "\n",
        "Most LLMs are a **stack of identical Transformer blocks**. Each block fuses two main components:\n",
        "\n",
        "| Step | What it does | Where it lives in code |\n",
        "|------|--------------|------------------------|\n",
        "| **Multi-Head Self-Attention** | Every token looks at every other token and decides *what matters*. | `block.attn` |\n",
        "| **Feed-Forward Network (MLP)** | Re-mixes information token-by-token. | `block.mlp` |\n",
        "\n",
        "Below, we load the smallest public GPT-2 (124 M parameters), grab its *first* block, and inspect the pieces.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "47c87f6e",
      "metadata": {
        "id": "47c87f6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ln_1    → LayerNorm\n",
            "attn    → GPT2Attention\n",
            "ln_2    → LayerNorm\n",
            "mlp     → GPT2MLP\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "# Load the 124 M-parameter GPT-2 and inspect its layers (12 layers)\n",
        "gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "block = gpt2.transformer.h[0] # GPT-2 has 12 such layers\n",
        "for name, module in block.named_children():\n",
        "    print(f\"{name:7s} → {module.__class__.__name__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "92df06df",
      "metadata": {
        "id": "92df06df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Output shape : torch.Size([1, 8, 768])\n"
          ]
        }
      ],
      "source": [
        "# Run a tiny forward pass through the first block\n",
        "seq_len = 8\n",
        "dummy_tokens = torch.randint(0, gpt2.config.vocab_size, (1, seq_len))\n",
        "with torch.no_grad():\n",
        "    # Embed tokens + positions the same way GPT-2 does\n",
        "    hidden = (\n",
        "        gpt2.transformer.wte(dummy_tokens) +\n",
        "        gpt2.transformer.wpe(torch.arange(seq_len))\n",
        "    )\n",
        "    # Forward through one layer\n",
        "    out = block(hidden, layer_past=None, use_cache=False)[0]\n",
        "print(\"\\nOutput shape :\", out.shape) # (batch, seq_len, hidden_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8493ecc8",
      "metadata": {
        "id": "8493ecc8"
      },
      "source": [
        "### 2.3 - Inside GPT-2\n",
        "\n",
        "GPT-2 is just many of those modules arranged in a repeating *block*. Let's print the modules inside the Transformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "a78ddee1",
      "metadata": {
        "id": "a78ddee1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wte     → Embedding\n",
            "wpe     → Embedding\n",
            "drop    → Dropout\n",
            "h       → ModuleList\n",
            "ln_f    → LayerNorm\n"
          ]
        }
      ],
      "source": [
        "# Print the name and modules inside gpt2\n",
        "for name, module in gpt2.transformer.named_children():\n",
        "    print(f\"{name:7s} → {module.__class__.__name__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed029847",
      "metadata": {
        "id": "ed029847"
      },
      "source": [
        "As you can see, the Transformer holds various modules, arranged from a list of blocks (`h`). The following table summarizes these modules:\n",
        "\n",
        "| Step | What it does | Why it matters |\n",
        "|------|--------------|----------------|\n",
        "| **Token → Embedding** | Converts IDs to vectors | Gives the model a numeric “handle” on words |\n",
        "| **Positional Encoding** | Adds “where am I?” info | Order matters in language |\n",
        "| **Multi-Head Self-Attention** | Each token asks “which other tokens should I look at?” | Lets the model relate words across a sentence |\n",
        "| **Feed-Forward Network** | Two stacked Linear layers with a non-linearity | Mixes information and adds depth |\n",
        "| **LayerNorm & Residual** | Stabilize training and help gradients flow | Keeps very deep networks trainable |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a6a7495",
      "metadata": {
        "id": "0a6a7495"
      },
      "source": [
        "### 2.4 LLM's output\n",
        "\n",
        "Passing a token sequence through an **LLM** yields a tensor of **logits** with shape  \n",
        "`(batch_size, seq_len, vocab_size)`.  \n",
        "Applying `softmax` on the last dimension turns those logits into probabilities.\n",
        "\n",
        "The cell below feeds an 8-token dummy sequence, prints the logits shape, and shows the five most likely next tokens for the final position.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "f98b7b34",
      "metadata": {
        "id": "f98b7b34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logits shape : torch.Size([1, 3, 50257])\n",
            "\n",
            "Top-5 predictions for the next token:\n",
            "        is  —  0.7773\n",
            "         ,  —  0.0373\n",
            "        's  —  0.0332\n",
            "       was  —  0.0127\n",
            "       and  —  0.0076\n"
          ]
        }
      ],
      "source": [
        "import torch, torch.nn.functional as F\n",
        "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
        "\n",
        "# Load gpt2 model and tokenizer\n",
        "\n",
        "try:\n",
        "    gpt2\n",
        "except NameError:\n",
        "    gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Tokenize input text\n",
        "text = \"Hello my name\"\n",
        "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids  # shape: (1, seq_len)\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = gpt2(input_ids).logits          # (1, seq_len, vocab_size)\n",
        "\n",
        "print(\"Logits shape :\", logits.shape)\n",
        "\n",
        "# Predict next token\n",
        "probs = F.softmax(logits[0, -1], dim=-1)     # (vocab_size,)\n",
        "topk = torch.topk(probs, 5)\n",
        "\n",
        "print(\"\\nTop-5 predictions for the next token:\")\n",
        "for idx, p in zip(topk.indices.tolist(), topk.values.tolist()):\n",
        "    print(f\"{tokenizer.decode([idx]):>10s}  —  {p:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0eb05c9b",
      "metadata": {
        "id": "0eb05c9b"
      },
      "source": [
        "### 2.5 - Key Takeaway\n",
        "\n",
        "A language model is nothing mystical: it’s a *huge composition* of small, understandable layers trained to predict the next token in a sequence of tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0ccf391",
      "metadata": {
        "id": "e0ccf391"
      },
      "source": [
        "# 3 - Generation\n",
        "Once an LLM is trained to predict the probabilities, we can generate text from the model. This process is called decoding or sampling.\n",
        "\n",
        "At each step, the LLM outputs a **probability distribution** over the next token. It is the job of the decoding algorithm to pick the next token, and move on to the next token. There are different decoding algorithms and hyper-parameters to control the generaiton:\n",
        "* **Greedy** → pick the single highest‑probability token each step (safe but repetitive).  \n",
        "* **Top‑k / Nucleus (top‑p)** → sample from a subset of likely tokens (adds variety).\n",
        "* **beam** -> applies beam search to pick tokens\n",
        "* **Temperature** → a *creativity* knob. Higher values flatten the probability distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac0c5728",
      "metadata": {
        "id": "ac0c5728"
      },
      "source": [
        "### 3.1 - Greedy decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "2f2cb953",
      "metadata": {
        "id": "2f2cb953"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded gpt2 as gpt2\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "MODELS = {\n",
        "    \"gpt2\": \"gpt2\",\n",
        "}\n",
        "tokenizers, models = {}, {}\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "for key, mid in MODELS.items():\n",
        "    tok = AutoTokenizer.from_pretrained(mid)\n",
        "    mdl = AutoModelForCausalLM.from_pretrained(mid).eval().to(device)\n",
        "    if tok.pad_token is None:\n",
        "        tok.pad_token = tok.eos_token\n",
        "    mdl.config.pad_token_id = tok.pad_token_id\n",
        "    tokenizers[key], models[key] = tok, mdl\n",
        "    print(f\"Loaded {mid} as {key}\")\n",
        "\n",
        "def generate(model_key, prompt, strategy=\"greedy\", max_new_tokens=100):\n",
        "    tok, mdl = tokenizers[model_key], models[model_key]\n",
        "    enc = tok(prompt, return_tensors=\"pt\").to(mdl.device)\n",
        "    gen_args = dict(**enc, max_new_tokens=max_new_tokens, pad_token_id=tok.pad_token_id)\n",
        "    if strategy==\"greedy\":\n",
        "        gen_args[\"do_sample\"]=False\n",
        "    elif strategy==\"top_k\":\n",
        "        gen_args.update(dict(do_sample=True, top_k=50, temperature=0.9))\n",
        "    elif strategy==\"top_p\":\n",
        "        gen_args.update(dict(do_sample=True, top_p=0.9, temperature=0.9))\n",
        "    out = mdl.generate(**gen_args)\n",
        "    return tok.decode(out[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "dbe777ba",
      "metadata": {
        "id": "dbe777ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "== GPT-2 | Greedy ==\n",
            "Who is Arnold?\n",
            "\n",
            "Arnold Schwarzenegger is a man who has been a man for over 30 years. He is a man who has been a man for over 30 years. He is a man who has been a man for over 30 years. He is a man who has been a man for over 30 years. He is a man who has been a man for over 30 years. He is a man who\n",
            "\n",
            "== GPT-2 | Greedy ==\n",
            "How old is the moon?\n",
            "\n",
            "The moon is the brightest star in the sky. It is the brightest star in the sky. It is the brightest star in the sky. It is the brightest star in the sky. It is the brightest star in the sky. It is the brightest star in the sky. It is the brightest star in the sky. It is the brightest star in the sky. It is the brightest star\n",
            "\n",
            "== GPT-2 | Greedy ==\n",
            "Suggest a party theme.\n",
            "\n",
            "The party theme is a simple, simple, and fun way to get your friends to join you.\n",
            "\n",
            "The party theme is a simple, simple, and fun way to get your friends to join you. The party theme is a simple, simple, and fun way to get your friends to join you. The party theme is a simple, simple, and fun way to get your friends\n"
          ]
        }
      ],
      "source": [
        "tests=[\"Who is Arnold?\",\"How old is the moon?\", \"Suggest a party theme.\"]\n",
        "for prompt in tests:\n",
        "    print(f\"\\n== GPT-2 | Greedy ==\")\n",
        "    print(generate(\"gpt2\", prompt, \"greedy\", 80))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f51d44b2",
      "metadata": {
        "id": "f51d44b2"
      },
      "source": [
        "\n",
        "Naively picking the single best token every time has the following issues in practice:\n",
        "\n",
        "* **Loop**: “The cat is is is…”  \n",
        "* **Miss long-term payoff**: the highest-probability word *now* might paint you into a boring corner later."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91607661",
      "metadata": {
        "id": "91607661"
      },
      "source": [
        "### 3.2 - Top-k or top-p sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "0633d4a3",
      "metadata": {
        "id": "0633d4a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "== GPT-2 | Top-k ==\n",
            "Who is Arnold?\n",
            "\n",
            "Arnold Schwarzenegger is a man who has been a man for over 30 years. He is a man who has been a man for over 30 years. He is a man who has been a man for over 30 years. He is a\n",
            "\n",
            "== GPT-2 | Top-p ==\n",
            "Who is Arnold?\n",
            "\n",
            "Arnold Schwarzenegger is a man who has been a man for over 30 years. He is a man who has been a man for over 30 years. He is a man who has been\n",
            "\n",
            "== GPT-2 | Beam ==\n",
            "Who is Arnold?\n",
            "\n",
            "Arnold Schwarzenegger is a man who has been a man for over 30 years. He is a man who has been a man for over 30 years. He is a man who has been\n",
            "\n",
            "== GPT-2 | Top-k ==\n",
            "How old is the moon?\n",
            "\n",
            "The moon is the brightest star in the sky. It is the brightest star in the sky. It is the brightest star in the sky. It is the brightest star in the sky. It is the brightest star in the sky. It is\n",
            "\n",
            "== GPT-2 | Top-p ==\n",
            "How old is the moon?\n",
            "\n",
            "The moon is the brightest star in the sky. It is the brightest star in the sky. It is the brightest star in the sky. It is the brightest star in the sky. It\n",
            "\n",
            "== GPT-2 | Beam ==\n",
            "How old is the moon?\n",
            "\n",
            "The moon is the brightest star in the sky. It is the brightest star in the sky. It is the brightest star in the sky. It is the brightest star in the sky. It\n",
            "\n",
            "== GPT-2 | Top-k ==\n",
            "Suggest a party theme.\n",
            "\n",
            "The party theme is a simple, simple, and fun way to get your friends to join you.\n",
            "\n",
            "The party theme is a simple, simple, and fun way to get your friends to join you. The party theme is a simple\n",
            "\n",
            "== GPT-2 | Top-p ==\n",
            "Suggest a party theme.\n",
            "\n",
            "The party theme is a simple, simple, and fun way to get your friends to join you.\n",
            "\n",
            "The party theme is a simple, simple, and fun way to get your friends\n",
            "\n",
            "== GPT-2 | Beam ==\n",
            "Suggest a party theme.\n",
            "\n",
            "The party theme is a simple, simple, and fun way to get your friends to join you.\n",
            "\n",
            "The party theme is a simple, simple, and fun way to get your friends\n"
          ]
        }
      ],
      "source": [
        "\n",
        "tests=[\"Who is Arnold?\",\"How old is the moon?\", \"Suggest a party theme.\"]\n",
        "for prompt in tests:\n",
        "    print(f\"\\n== GPT-2 | Top-k ==\")\n",
        "    print(generate(\"gpt2\", prompt, \"top-k\", 40))\n",
        "\n",
        "    print(f\"\\n== GPT-2 | Top-p ==\")\n",
        "    print(generate(\"gpt2\", prompt, \"top-p\", 40))\n",
        "\n",
        "    print(f\"\\n== GPT-2 | Beam ==\")\n",
        "    print(generate(\"gpt2\", prompt, \"beam\", 40))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "004b4039",
      "metadata": {
        "id": "004b4039"
      },
      "source": [
        "### 3.3 - Try It Yourself\n",
        "\n",
        "1. Scroll to the list called `tests`.\n",
        "2. Swap in your own prompts or tweak the decoding strategy.  \n",
        "3. Re‑run the cell and compare the vibes.\n",
        "\n",
        "> **Tip:** Try the same prompt with `greedy` vs. `top_p` (0.9) and see how the tone changes. Notice especially how small temperature tweaks can soften or sharpen the prose.\n",
        "\n",
        "* `strategy`: `\"greedy\"`, `\"beam\"`, `\"top_k\"`, `\"top_p\"`  \n",
        "* `temperature`: `0.2 – 2.0`  \n",
        "* `k` or `p` thresholds\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b775b02",
      "metadata": {
        "id": "6b775b02"
      },
      "source": [
        "# 4 - Completion vs. Instruction-tuned LLMs\n",
        "\n",
        "We have seen that we can use GPT2 model to pass an input text and generate a new text. However, this model only continues the provided text. It is not engaging in a dialouge-like conversation and cannot be helpful by answering instructions. On the other hand, **instruction-tuned LLMs** like `Qwen-Chat` go through an extra training stage called **post-training** after the base “completion” model is finished. Because of post-training step, an instruction-tuned LLM will:\n",
        "\n",
        "* **Read the entire prompt as a request,** not just as text to mimic.  \n",
        "* **Stay in dialogue mode**. Answer questions, follow steps, ask clarifying queries.  \n",
        "* **Refuse or safe-complete** when instructions are unsafe or disallowed.  \n",
        "* **Adopt a consistent persona** (e.g., “Assistant”) rather than drifting into story continuation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1706dc08",
      "metadata": {
        "id": "1706dc08"
      },
      "source": [
        "### 4.1 - Qwen1.5-8B vs. GPT2\n",
        "\n",
        "In the code below we’ll feed the same prompt to:\n",
        "\n",
        "* **GPT-2 (completion-only)** – it will simply keep writing in the same style.  \n",
        "* **Qwen-Chat (instruction-tuned)** – it will obey the instruction and respond directly.\n",
        "\n",
        "Comparing the two outputs makes the difference easy to see."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "57b73e7a",
      "metadata": {
        "id": "57b73e7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded gpt2 as gpt2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded Qwen/Qwen1.5-1.8B-Chat as qwen\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "MODELS = {\n",
        "    \"gpt2\": \"gpt2\",\n",
        "    \"qwen\": \"Qwen/Qwen1.5-1.8B-Chat\"\n",
        "}\n",
        "tokenizers, models = {}, {}\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "for key, mid in MODELS.items():\n",
        "    tok = AutoTokenizer.from_pretrained(mid)\n",
        "    mdl = AutoModelForCausalLM.from_pretrained(mid).eval().to(device)\n",
        "    if tok.pad_token is None:\n",
        "        tok.pad_token = tok.eos_token\n",
        "    mdl.config.pad_token_id = tok.pad_token_id\n",
        "    tokenizers[key], models[key] = tok, mdl\n",
        "    print(f\"Loaded {mid} as {key}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef49ab1b",
      "metadata": {
        "id": "ef49ab1b"
      },
      "source": [
        "\n",
        "We downloaded two tiny checkpoints: `GPT‑2` (124 M parameters) and `Qwen‑1.5‑Chat` (1.8 B). If the cell took a while, that was mostly network time. Models are stored locally after the first run.\n",
        "\n",
        "Let's now generate text and compare two models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "0c78a508",
      "metadata": {
        "id": "0c78a508"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "== GPT2 | greedy ==\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and\n",
            "\n",
            "== QWEN | greedy ==\n",
            "Once upon a time, there was a young girl named Lily who lived in a small village nestled among the rolling hills of her home. Lily had always been fascinated by nature and spent most of her days exploring the surrounding forests, streams, and meadows. She loved to sing songs about the birds that sang in the trees, the flowers that bloomed in the fields, and the animals that roamed free.\n",
            "One day\n",
            "\n",
            "== GPT2 | top_k ==\n",
            "What is 2+2? The answer is pretty simple. Yes, you could ask me why it's 2+2, but you must remember that, because 2+2 is a very simple definition, you will not get that.\n",
            "\n",
            "Let's say that the first 2 letters of the alphabet are 2+2 (the two letters are NOT 2 as they are different). For example, if you are using the following two\n",
            "\n",
            "== QWEN | top_k ==\n",
            "What is 2+2? The sum of 2 and 2 is 4.\n",
            "\n",
            "== GPT2 | top_p ==\n",
            "Suggest a party theme. We've added that to our theme pack!\n",
            "\n",
            "Check out our list of themes and add them to your game!\n",
            "\n",
            "Get started\n",
            "\n",
            "There's a lot of cool stuff you can do with the free version of our theme pack. You can also share with your friends!\n",
            "\n",
            "Download\n",
            "\n",
            "The base theme pack can be downloaded for free to your system and installed on your computer.\n",
            "\n",
            "== QWEN | top_p ==\n",
            "Suggest a party theme. Title: \"复古星空之夜\"\n",
            "\n",
            "Description:\n",
            "\"The复古星空之夜\" party theme is inspired by the mysterious and celestial beauty of space and the nostalgic charm of the 1960s. The event will transport guests to an era when stars shone bright, rockets flew high, and science fiction was at the forefront of popular culture.\n",
            "\n",
            "Theme Elements:\n",
            "\n",
            "1. Décor:\n",
            "   - Decorative\n"
          ]
        }
      ],
      "source": [
        "\n",
        "tests=[(\"Once upon a time\",\"greedy\"),(\"What is 2+2?\",\"top_k\"),(\"Suggest a party theme.\",\"top_p\")]\n",
        "for prompt,strategy in tests:\n",
        "    for key in [\"gpt2\",\"qwen\"]:\n",
        "        print(f\"\\n== {key.upper()} | {strategy} ==\")\n",
        "        print(generate(key,prompt,strategy,80))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e1c3da1",
      "metadata": {
        "id": "8e1c3da1"
      },
      "source": [
        "# 5. (Optional) A Small LLM Playground"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "313ba974",
      "metadata": {
        "id": "313ba974"
      },
      "source": [
        "### 5.1 ‑ Interactive Playground\n",
        "\n",
        "Enter a prompt, pick a model and decoding strategy, adjust the temperature, and press **Generate** to watch the model respond.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a67a884",
      "metadata": {
        "id": "1a67a884"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cc748aba7fa2492da4aeb4f4c2140f2a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Textarea(value='Tell me a fun fact about space.', description='Prompt:', layout=Layout(height='…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Make sure models and tokenizers are loaded\n",
        "try:\n",
        "    tokenizers\n",
        "    models\n",
        "except NameError:\n",
        "    raise RuntimeError(\"Please run the earlier setup cells that load the models before using the playground.\")\n",
        "\n",
        "def generate_playground(model_key, prompt, strategy=\"greedy\", temperature=1.0, max_new_tokens=100):\n",
        "    tok, mdl = tokenizers[model_key], models[model_key]\n",
        "    enc = tok(prompt, return_tensors=\"pt\").to(mdl.device)\n",
        "    gen_args = dict(**enc, max_new_tokens=max_new_tokens, pad_token_id=tok.pad_token_id)\n",
        "    if strategy == \"greedy\":\n",
        "        gen_args[\"do_sample\"] = False\n",
        "    elif strategy == \"top_k\":\n",
        "        gen_args.update(dict(do_sample=True, top_k=50, temperature=temperature))\n",
        "    elif strategy == \"top_p\":\n",
        "        gen_args.update(dict(do_sample=True, top_p=0.9, temperature=temperature))\n",
        "    else:\n",
        "        raise ValueError(\"Unknown strategy\")\n",
        "    out = mdl.generate(**gen_args)\n",
        "    return tok.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "prompt_box = widgets.Textarea(\n",
        "    value=\"Tell me a fun fact about space.\",\n",
        "    placeholder=\"Type your prompt here\",\n",
        "    description=\"Prompt:\",\n",
        "    layout=widgets.Layout(width=\"100%\", height=\"120px\")\n",
        ")\n",
        "\n",
        "model_dropdown = widgets.Dropdown(\n",
        "    options=[(\"GPT‑2\", \"gpt2\"), (\"Qwen‑1.5‑Chat\", \"qwen\")],\n",
        "    value=\"gpt2\",\n",
        "    description=\"Model:\"\n",
        ")\n",
        "\n",
        "strategy_dropdown = widgets.Dropdown(\n",
        "    options=[(\"Greedy\", \"greedy\"), (\"Top‑k\", \"top_k\"), (\"Top‑p\", \"top_p\")],\n",
        "    value=\"greedy\",\n",
        "    description=\"Strategy:\"\n",
        ")\n",
        "\n",
        "temperature_slider = widgets.FloatSlider(\n",
        "    value=1.0,\n",
        "    min=0.1,\n",
        "    max=2.0,\n",
        "    step=0.1,\n",
        "    description=\"Temp:\"\n",
        ")\n",
        "\n",
        "generate_button = widgets.Button(description=\"Generate\", button_style=\"primary\")\n",
        "output_area = widgets.Output()\n",
        "\n",
        "def on_generate(_):\n",
        "    output_area.clear_output()\n",
        "    with output_area:\n",
        "        try:\n",
        "            result = generate_playground(\n",
        "                model_dropdown.value,\n",
        "                prompt_box.value,\n",
        "                strategy_dropdown.value,\n",
        "                temperature_slider.value\n",
        "            )\n",
        "            display(Markdown(f\"**Output:**\\n\\n{result}\"))\n",
        "        except Exception as e:\n",
        "            print(\"Error:\", e)\n",
        "\n",
        "generate_button.on_click(on_generate)\n",
        "\n",
        "ui = widgets.VBox([\n",
        "    prompt_box,\n",
        "    widgets.HBox([model_dropdown, strategy_dropdown, temperature_slider]),\n",
        "    generate_button,\n",
        "    output_area\n",
        "])\n",
        "\n",
        "display(ui)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfbccead",
      "metadata": {
        "id": "cfbccead"
      },
      "source": [
        "\n",
        "## 🎉 Congratulations!\n",
        "\n",
        "You’ve just learned, explored, and inspected a real **LLM**. In one project you:\n",
        "* Learned how **tokenization** works in practice\n",
        "* Used `tiktoken` library to load and experiment with most advanced tokenizers.\n",
        "* Explored LLM architecture and inspected GPT2 blocks and layers\n",
        "* Learned decoding strategies and used `top-p` to generate text from GPT2\n",
        "* Loaded a powerful chat model, `Qwen1.5-8B` and generated text\n",
        "* Built an LLM playground\n",
        "\n",
        "\n",
        "👏 **Great job!** Take a moment to celebrate. You now have a working mental model of how LLMs work. The skills you used here power most LLMs you see everywhere.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
